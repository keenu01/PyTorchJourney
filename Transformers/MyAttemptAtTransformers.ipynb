{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "157a7ed218bb4effbe5b8c5595f530b0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_7f248edf00354cea8a67ecd74457c812",
              "IPY_MODEL_a557bdef1c054e6e9abc4f84be2ddb6d",
              "IPY_MODEL_09440ab6ee20495bb66d6bf43824229b"
            ],
            "layout": "IPY_MODEL_7755c3d2929141af987370ee23b94f70"
          }
        },
        "7f248edf00354cea8a67ecd74457c812": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ff3307c0dcf94df7ac0e28efcc9134cd",
            "placeholder": "​",
            "style": "IPY_MODEL_e88233e03d3849dd9dc6a1e6a1c349a8",
            "value": "100%"
          }
        },
        "a557bdef1c054e6e9abc4f84be2ddb6d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_dbaf2cfdaad24e409bd0553c7fe91238",
            "max": 1000,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_9803fb0742544005b460eb02c0a9a0f0",
            "value": 1000
          }
        },
        "09440ab6ee20495bb66d6bf43824229b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_57c5cf46b1fc44f39e641861e70a0198",
            "placeholder": "​",
            "style": "IPY_MODEL_7ea854867e474a7ba9f73ac59deef013",
            "value": " 1000/1000 [07:35&lt;00:00,  2.26it/s]"
          }
        },
        "7755c3d2929141af987370ee23b94f70": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ff3307c0dcf94df7ac0e28efcc9134cd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e88233e03d3849dd9dc6a1e6a1c349a8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "dbaf2cfdaad24e409bd0553c7fe91238": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9803fb0742544005b460eb02c0a9a0f0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "57c5cf46b1fc44f39e641861e70a0198": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7ea854867e474a7ba9f73ac59deef013": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Dater Preperation and Analysis\n"
      ],
      "metadata": {
        "id": "OggRsYJ-FZDP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch as t\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as f\n",
        "import random as r"
      ],
      "metadata": {
        "id": "lgYhKq1tV8iD"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6vOJLIp_aeNT",
        "outputId": "38f50f65-6aa9-43ca-88ce-7a2e16b7de76"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-06-06 15:18:57--  https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.109.133, 185.199.110.133, 185.199.111.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.109.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1115394 (1.1M) [text/plain]\n",
            "Saving to: ‘input.txt’\n",
            "\n",
            "\rinput.txt             0%[                    ]       0  --.-KB/s               \rinput.txt           100%[===================>]   1.06M  --.-KB/s    in 0.06s   \n",
            "\n",
            "2024-06-06 15:18:57 (17.8 MB/s) - ‘input.txt’ saved [1115394/1115394]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 64 # how many independent sequences will we process in parallel?\n",
        "block_size = 256 # what is the maximum context length for predictions?\n",
        "max_iters = 5000\n",
        "eval_interval = 500\n",
        "learning_rate = 3e-4\n",
        "eval_iters = 200\n",
        "n_embd = 384\n",
        "n_head = 6\n",
        "n_layer = 6\n",
        "dropout = 0.2"
      ],
      "metadata": {
        "id": "9wP0w1UjIl0k"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"input.txt\",\"r\") as i:\n",
        "  text = i.read()\n",
        "  print(f\"Length:{len(text)} \\n\\nFirst 100 Text:\\n\\n{text[:1000]}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7BXFWMjNlBPh",
        "outputId": "2b19ab88-82ce-4800-bfcc-8a952d58adec"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Length:1115394 \n",
            "\n",
            "First 100 Text:\n",
            "\n",
            "First Citizen:\n",
            "Before we proceed any further, hear me speak.\n",
            "\n",
            "All:\n",
            "Speak, speak.\n",
            "\n",
            "First Citizen:\n",
            "You are all resolved rather to die than to famish?\n",
            "\n",
            "All:\n",
            "Resolved. resolved.\n",
            "\n",
            "First Citizen:\n",
            "First, you know Caius Marcius is chief enemy to the people.\n",
            "\n",
            "All:\n",
            "We know't, we know't.\n",
            "\n",
            "First Citizen:\n",
            "Let us kill him, and we'll have corn at our own price.\n",
            "Is't a verdict?\n",
            "\n",
            "All:\n",
            "No more talking on't; let it be done: away, away!\n",
            "\n",
            "Second Citizen:\n",
            "One word, good citizens.\n",
            "\n",
            "First Citizen:\n",
            "We are accounted poor citizens, the patricians good.\n",
            "What authority surfeits on would relieve us: if they\n",
            "would yield us but the superfluity, while it were\n",
            "wholesome, we might guess they relieved us humanely;\n",
            "but they think we are too dear: the leanness that\n",
            "afflicts us, the object of our misery, is as an\n",
            "inventory to particularise their abundance; our\n",
            "sufferance is a gain to them Let us revenge this with\n",
            "our pikes, ere we become rakes: for the gods know I\n",
            "speak this in hunger for bread, not in thirst for revenge.\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "chars = sorted(list(set(text)))\n",
        "vocab_size = len(chars)\n",
        "\n",
        "\n",
        "print(\"\".join(chars))\n",
        "print(f\"Length of Characters:{vocab_size}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CepevI5hlznY",
        "outputId": "216d8391-b267-4d14-f330-66a4691906ae"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " !$&',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\n",
            "Length of Characters:65\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Creating encoder and decoder\n",
        "\n",
        "stoi = { ch:i for i,ch in enumerate(chars)}\n",
        "itos = { i:ch for i,ch in enumerate(chars)}\n",
        "encode = lambda s:[stoi[c] for c in s]\n",
        "decode = lambda l: ''.join([itos[i]for i in l])\n",
        "\n",
        "\n",
        "User = input(\"Enter something to be encoded \")\n",
        "print(decode(encode(User)))\n",
        "print(encode(User))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5gnAOFOEmImA",
        "outputId": "8d5f0f0b-8d48-45f2-90c8-ab3c924fcc8a"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter something to be encoded Hello i am keenu!\n",
            "Hello i am keenu!\n",
            "[20, 43, 50, 50, 53, 1, 47, 1, 39, 51, 1, 49, 43, 43, 52, 59, 2]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "data = t.tensor(encode(text),dtype=t.long)\n",
        "#creating testing split\n",
        "\n",
        "split = int(0.9*len(data))\n",
        "train_data = data[split:]\n",
        "test_data = data[:split]\n",
        "\n",
        "train_data.shape,test_data.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "693gEBq3mlTE",
        "outputId": "bc08323b-ded3-43f7-b138-331a4c3e047e"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([111540]), torch.Size([1003854]))"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 4\n",
        "block_size = 8\n",
        "val_data =0\n",
        "def get_batch(split):\n",
        "  data = train_data if split == 'train' else val_data\n",
        "  ix = t.randint(len(data) - block_size, (batch_size,))\n",
        "  x = t.stack([data [i:i+block_size]for i in ix])\n",
        "  y = t.stack([data[i+1:i+block_size+1]for i in ix])\n",
        "  return x,y\n",
        "\n",
        "xb,yb = get_batch('train')\n",
        "print(\"==================\")\n",
        "print(\"Input:\")\n",
        "print(xb)\n",
        "print(\"Target:\")\n",
        "print(yb)\n",
        "print(\"=================\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sMlA6LyTXF55",
        "outputId": "4bb2eb2b-4c2f-47d0-aef4-fdfd1c8d4e4f"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==================\n",
            "Input:\n",
            "tensor([[43, 43,  6,  1, 51, 39, 57, 58],\n",
            "        [26, 16, 13, 10,  0, 31, 47, 56],\n",
            "        [ 1, 44, 53, 56,  1, 61, 46, 63],\n",
            "        [21,  1, 57, 39, 61,  1, 57, 59]])\n",
            "Target:\n",
            "tensor([[43,  6,  1, 51, 39, 57, 58, 43],\n",
            "        [16, 13, 10,  0, 31, 47, 56,  6],\n",
            "        [44, 53, 56,  1, 61, 46, 63,  6],\n",
            "        [ 1, 57, 39, 61,  1, 57, 59, 44]])\n",
            "=================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "block_size = 8\n",
        "\n",
        "x = train_data[:block_size]\n",
        "y = train_data[1:block_size+1]\n",
        "\n",
        "for t in range(block_size):\n",
        "  context = x[:t+1]\n",
        "  target = y[t]\n",
        "  print(f\"When input is {context} the target: {target}\")\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_iKzY5eaqccE",
        "outputId": "b93bfcc8-ae6d-4655-e1f4-085c4bcc4695"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "When input is tensor([12]) the target: 0\n",
            "When input is tensor([12,  0]) the target: 0\n",
            "When input is tensor([12,  0,  0]) the target: 19\n",
            "When input is tensor([12,  0,  0, 19]) the target: 30\n",
            "When input is tensor([12,  0,  0, 19, 30]) the target: 17\n",
            "When input is tensor([12,  0,  0, 19, 30, 17]) the target: 25\n",
            "When input is tensor([12,  0,  0, 19, 30, 17, 25]) the target: 21\n",
            "When input is tensor([12,  0,  0, 19, 30, 17, 25, 21]) the target: 27\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## BigramLanguageModel\n",
        "\n",
        "# A super basic baseline for our GPT (i will make it mroe sigma later)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "9UkIgiEdecCv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch as t\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as f\n",
        "import random as r\n",
        "\n",
        "class BigramLanguageModel(nn.Module):\n",
        "\n",
        "  def __init__(self,vocab_size:int):\n",
        "    super().__init__()\n",
        "    self.token_embedding_table = nn.Embedding(vocab_size,vocab_size)\n",
        "\n",
        "\n",
        "  def forward(self,idx,targets=None):\n",
        "     logits = self.token_embedding_table(idx)\n",
        "     if targets is None:\n",
        "        loss = None\n",
        "     else:\n",
        "        B,T,C = logits.shape\n",
        "        logits = logits.view(B*T,C)\n",
        "        targets = targets.view(B*T)\n",
        "        loss = f.cross_entropy(logits,targets)\n",
        "\n",
        "     return logits,loss\n",
        "\n",
        "  def generate(self,idx,max_new_tokens):\n",
        "    for _ in range(max_new_tokens):\n",
        "      logits , loss = self(idx)\n",
        "\n",
        "      logits = logits[:,-1,:]\n",
        "\n",
        "      probs = f.softmax(logits,dim=-1)\n",
        "\n",
        "      idx_next = t.multinomial(probs,num_samples=1)\n",
        "\n",
        "      idx = t.cat((idx,idx_next),dim=1)\n",
        "    return idx"
      ],
      "metadata": {
        "id": "lcUVQAmOugiZ"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "t.manual_seed(42)\n",
        "model = BigramLanguageModel(vocab_size)\n",
        "\n",
        "\n",
        "logits ,loss = model(xb,yb)\n",
        "print(logits.shape)\n",
        "print(f\"{loss*100}%\")\n",
        "\n",
        "#How to acc get shit from the Model.\n",
        "print(decode(model.generate(idx = t.zeros((1,1),dtype=t.long),max_new_tokens=100)[0].tolist()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nd_un6vbWeDR",
        "outputId": "96a0f8a4-0db0-4a9d-8a1e-14d0ad4620a9"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([32, 65])\n",
            "463.4061279296875%\n",
            "\n",
            "uoiaF$z\n",
            "M?kI;h\n",
            "DbuMG,H3LYNmrDxKgTpvAKOF-jU.hc;fBMTGa-IS\n",
            "g3lEb&ZQ,l;:m;lpcNN\n",
            "KpVEYRIIM,'hCRbMAcWTkrnH\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = t.optim.Adam(model.parameters(),lr=1e-3)"
      ],
      "metadata": {
        "id": "Fv6yt58hc5lj"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size  = 32\n",
        "EPOCHS = 10000\n",
        "val_data = test_data  # Assuming val_data is intended to be the test split\n",
        "per_epoch = 1000\n",
        "for epoch in range(EPOCHS):\n",
        "  xb,yb = get_batch('train')\n",
        "  logits,loss = model(xb,yb)\n",
        "  optimizer.zero_grad(set_to_none=True)\n",
        "  loss.backward()\n",
        "  optimizer.step()\n",
        "  if epoch % per_epoch == 0:\n",
        "    print(f\"epoch||{epoch}||Loss:{loss.item()}\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "swXBEK6bdAum",
        "outputId": "eb851ee7-694c-4709-ab45-3ded34461a1e"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch||0||Loss:4.823399543762207\n",
            "epoch||1000||Loss:3.857077121734619\n",
            "epoch||2000||Loss:3.1169354915618896\n",
            "epoch||3000||Loss:2.8038034439086914\n",
            "epoch||4000||Loss:2.5071682929992676\n",
            "epoch||5000||Loss:2.6767418384552\n",
            "epoch||6000||Loss:2.54191517829895\n",
            "epoch||7000||Loss:2.3395607471466064\n",
            "epoch||8000||Loss:2.4384925365448\n",
            "epoch||9000||Loss:2.48649263381958\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(decode(model.generate(idx = t.zeros((1,1),dtype=t.long),max_new_tokens=500)[0].tolist()))"
      ],
      "metadata": {
        "id": "WMvZLxJIdjhm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fc18db51-5d24-474e-9a7d-c66646ade6c7"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "SPes thagreait; mere, herer w war ha y velise 'tWhowe murfor add higug y wharod on!\n",
            "zDIRENTHiched owavente, m te kere isthin ISana earusher Tistced.\n",
            "\n",
            "Thire go!\n",
            "\n",
            "\n",
            "Whe hilly por btrouraisarastaldXfe!-motolicou mfeuthelalderou STIOSa por s bemue orinth'Top mofeawontill wintiryo d, DAh o p--healy wecayse hore itas; nd hillle d n,\n",
            "HItyouprryon an my t athisocorens f mucak tors outis tld, Scumu t, h wo, mataro thindodore ls?\n",
            "\n",
            "And? jo! me,\n",
            "ARANAPlloutherghes by jour;\n",
            "s th edi'cr pr a anthomitor!\n",
            "Rjat '\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "B,T,C=4,8,2\n",
        "\n",
        "x = t.randn(B,T,C)\n",
        "x"
      ],
      "metadata": {
        "id": "sf31eq6ydCSv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "81ff60c0-e87c-454f-c678-a1f20b6a9529"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[ 0.2239, -1.1799],\n",
              "         [-1.9887,  1.1181],\n",
              "         [ 0.6743, -0.0076],\n",
              "         [ 1.5443,  0.6219],\n",
              "         [ 0.5896, -1.6634],\n",
              "         [ 1.2190, -0.4100],\n",
              "         [-0.2676,  1.4030],\n",
              "         [-0.5201,  0.3038]],\n",
              "\n",
              "        [[ 0.2175,  1.7635],\n",
              "         [-0.1765,  0.3155],\n",
              "         [-0.1744,  0.5800],\n",
              "         [ 1.7847,  2.3204],\n",
              "         [-2.3698, -1.7093],\n",
              "         [-1.0754, -0.3491],\n",
              "         [ 1.6318, -0.5260],\n",
              "         [-1.6368,  0.5406]],\n",
              "\n",
              "        [[-0.1789, -1.3413],\n",
              "         [-1.6388,  1.3759],\n",
              "         [-0.4973, -0.7836],\n",
              "         [ 0.7914, -0.0546],\n",
              "         [-0.3778,  0.5410],\n",
              "         [-1.2600, -0.2633],\n",
              "         [-0.8717, -1.4530],\n",
              "         [-1.4891, -1.1703]],\n",
              "\n",
              "        [[-0.9576, -0.9216],\n",
              "         [-1.2985,  1.5033],\n",
              "         [-0.7484, -0.2542],\n",
              "         [ 0.8630, -0.8835],\n",
              "         [ 0.2139, -1.1100],\n",
              "         [-0.8316,  2.6219],\n",
              "         [ 0.6223,  1.3762],\n",
              "         [-0.0905,  1.0469]]])"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "xbow = t.zeros((B,T,C))\n",
        "for b in range(B):\n",
        "  for t in range(T):\n",
        "    xprev = x[b,:t+1]\n",
        "    xbow[b,t]=t.mean(xprev,0)\n",
        "\n",
        "print(xbow)"
      ],
      "metadata": {
        "id": "rpduzVsygQ5J",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 210
        },
        "outputId": "71746933-71f9-4a7e-c14f-ab884ed4c10e"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "'int' object has no attribute 'mean'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-17-b055caca272d>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m   \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mxprev\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mxbow\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxprev\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxbow\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'int' object has no attribute 'mean'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "xbow[0]\n"
      ],
      "metadata": {
        "id": "IkicZh8ogRDY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "798528df-b0c2-432a-d775-1a6bad3f125c"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0., 0.],\n",
              "        [0., 0.],\n",
              "        [0., 0.],\n",
              "        [0., 0.],\n",
              "        [0., 0.],\n",
              "        [0., 0.],\n",
              "        [0., 0.],\n",
              "        [0., 0.]])"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## V2\n",
        "\n",
        "# A less basic transformer\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Q_J5qDSsnbuC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch as t\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as f\n",
        "import random as r\n",
        "\n"
      ],
      "metadata": {
        "id": "zbcVa3cgnbuM"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch as t\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "import random as r\n",
        "\n"
      ],
      "metadata": {
        "id": "RWde4MpMnbuM"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "B,T,C = 4,8,32\n",
        "\n",
        "x = t.randn(B,T,C)\n",
        "\n",
        "head_size = 16\n",
        "\n",
        "key = nn.Linear(C,head_size,bias=False)\n",
        "query = nn.Linear(C,head_size,bias=False)\n",
        "value = nn.Linear(C,head_size,bias=False)\n",
        "\n",
        "\n",
        "k = key(x)\n",
        "q = query(x)\n",
        "wei = q @ k.transpose(-2,-1)\n",
        "\n",
        "tril = t.tril(t.ones(T,T))\n",
        "\n",
        "wei = t.zeros((T,T))\n",
        "#Allow in Encoder Blocks (not in decoder Blocks)\n",
        "wei = wei.masked_fill(tril==0,float('-inf'))\n",
        "wei = f.softmax(wei,dim=-1)\n",
        "v = value(x)\n",
        "out = wei @ v\n",
        "\n",
        "out.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P6HoZXn4nbuN",
        "outputId": "60b14e5a-9a3e-4c1e-c926-3aaa30050cf6"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([4, 8, 16])"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Head(nn.Module):\n",
        "  def __init__(self,head_size):\n",
        "    super().__init__()\n",
        "    self.key = nn.Linear(n_embd,head_size,bias=False)\n",
        "    self.query = nn.Linear(n_embd,head_size,bias=False)\n",
        "    self.value = nn.Linear(n_embd,head_size,bias=False)\n",
        "    self.register_buffer('tril',t.tril(t.ones(block_size, block_size)))\n",
        "\n",
        "  def forward(self,x):\n",
        "    B,T,C = x.shape\n",
        "    k = self.key(x)\n",
        "    q = self.query(x)\n",
        "\n",
        "    wei = q @ k.transpose(-2,-1) * C**-0.5\n",
        "    wei = wei.masked_fill(self.tril[:T,:T] == 0, float('-inf'))\n",
        "    wei = F.softmax(wei,dim=-1)\n",
        "\n",
        "    v = self.value(x)\n",
        "    out = wei @ v\n",
        "    return out\n",
        "\n"
      ],
      "metadata": {
        "id": "wH7Id9JVnbuN"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "8eBQjuVjGW2F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "hed = Head(16)"
      ],
      "metadata": {
        "id": "y1FBe1n0nbuN"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class BigramLanguageModelV2(nn.Module):\n",
        "\n",
        "  def __init__(self,n_embed):\n",
        "    super().__init__()\n",
        "    self.token_embedding_table = nn.Embedding(vocab_size,n_embd)\n",
        "    self.postion_embedding_table = nn.Embedding(block_size,n_embd)\n",
        "    self.sa_head = Head(n_embd)\n",
        "    self.lm_head = nn.Linear(n_embd,vocab_size)\n",
        "\n",
        "\n",
        "  def forward(self,idx,targets=None):\n",
        "     B,T = idx.shape\n",
        "     tok_emb = self.token_embedding_table(idx)\n",
        "     pos_emb = self.postion_embedding_table(t.arange(T))\n",
        "     x = tok_emb + pos_emb\n",
        "     x = self.sa_head(x)\n",
        "     logits = self.sa_head(x)\n",
        "     if targets is None:\n",
        "        loss = None\n",
        "     else:\n",
        "        B,T,C = logits.shape\n",
        "        logits = logits.view(B*T,C)\n",
        "        targets = targets.view(B*T)\n",
        "        loss = f.cross_entropy(logits,targets)\n",
        "\n",
        "     return logits,loss\n",
        "\n",
        "\n",
        "  def generate(self,idx,max_new_tokens):\n",
        "    for _ in range(max_new_tokens):\n",
        "      idx_con = idx[:,-block_size:]\n",
        "      logits, loss = self[idx_con]\n",
        "\n",
        "      logits = logits[:,-1,1]\n",
        "      logits = F.softmax(logits,dim=1)\n",
        "      idx_next = t.multinomial(probs,num_samples=1)\n",
        "\n",
        "      idx=t.cat((idx,idx_next),dim=1)\n",
        "\n",
        "    return idx"
      ],
      "metadata": {
        "id": "RCfh7bPJnbuN"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "XrqfOLdEK9Rl"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = t.optim.Adam(model.parameters(),lr=1e-3)"
      ],
      "metadata": {
        "id": "KiSib-XVnbuN"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "t.manual_seed(42)\n",
        "batch_size  = 32\n",
        "model = BigramLanguageModelV2(100)\n",
        "EPOCHS = 100\n",
        "val_data = test_data  # Assuming val_data is intended to be the test split\n",
        "per_epoch = 10\n",
        "for epoch in range(EPOCHS):\n",
        "  model.train()\n",
        "  xb,yb = get_batch('train')\n",
        "  logits,loss = model(xb,yb)\n",
        "  optimizer.zero_grad(set_to_none=True)\n",
        "  loss.backward()\n",
        "  optimizer.step()\n",
        "  if epoch % per_epoch == 0:\n",
        "    print(f\"epoch||{epoch}||Loss:{loss.item()}\")\n",
        "  model.eval()\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G0AAYkd5nbuO",
        "outputId": "0cc4af5b-31b3-4e1a-ee8d-982446e6883e"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch||0||Loss:6.0034379959106445\n",
            "epoch||10||Loss:6.032484531402588\n",
            "epoch||20||Loss:6.0594987869262695\n",
            "epoch||30||Loss:6.029374599456787\n",
            "epoch||40||Loss:6.045790195465088\n",
            "epoch||50||Loss:6.02937650680542\n",
            "epoch||60||Loss:6.036391735076904\n",
            "epoch||70||Loss:6.041642189025879\n",
            "epoch||80||Loss:6.069952011108398\n",
            "epoch||90||Loss:6.036606311798096\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-4nfagBPbeBJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(decode(model.generate(idx = t.zeros((1,1),dtype=t.long),max_new_tokens=500)[0].tolist()))"
      ],
      "metadata": {
        "id": "kdR-f5xZKW4Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "  def __init__(self,num_heads,head_size):\n",
        "    super().__init__()\n",
        "    self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
        "\n",
        "  def forward(self,x):\n",
        "    return t.cat([h(x) for h in self.heads], dim = 1)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "LBJM7wrkKhWk"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class BigramLanguageModelV2(nn.Module):\n",
        "\n",
        "  def __init__(self,n_embed):\n",
        "    super().__init__()\n",
        "    self.token_embedding_table = nn.Embedding(vocab_size,n_embd)\n",
        "    self.postion_embedding_table = nn.Embedding(block_size,n_embd)\n",
        "    self.sa_head = MultiHeadAttention(4,n_embd//4)\n",
        "    self.lm_head = nn.Linear(n_embd,vocab_size)\n",
        "\n",
        "\n",
        "  def forward(self,idx,targets=None):\n",
        "     B,T = idx.shape\n",
        "     tok_emb = self.token_embedding_table(idx)\n",
        "     pos_emb = self.postion_embedding_table(t.arange(T))\n",
        "     x = tok_emb + pos_emb\n",
        "     x = self.sa_head(x)\n",
        "     logits = self.sa_head(x)\n",
        "     if targets is None:\n",
        "        loss = None\n",
        "     else:\n",
        "        B,T,C = logits.shape\n",
        "        logits = logits.view(B*T,C)\n",
        "        targets = targets.view(B*T)\n",
        "        loss = f.cross_entropy(logits,targets)\n",
        "\n",
        "     return logits,loss\n",
        "\n",
        "\n",
        "  def generate(self,idx,max_new_tokens):\n",
        "    for _ in range(max_new_tokens):\n",
        "      idx_con = idx[:,-block_size:]\n",
        "      logits, loss = self[idx_con]\n",
        "\n",
        "      logits = logits[:,-1,1]\n",
        "      logits = F.softmax(logits,dim=1)\n",
        "      idx_next = t.multinomial(probs,num_samples=1)\n",
        "\n",
        "      idx=t.cat((idx,idx_next),dim=1)\n",
        "\n",
        "    return idx"
      ],
      "metadata": {
        "id": "oKgqVsPaLKWw"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Head(nn.Module):\n",
        "    def __init__(self, head_size):\n",
        "        super().__init__()\n",
        "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, T, C = x.shape\n",
        "        k = self.key(x)\n",
        "        q = self.query(x)\n",
        "        v = self.value(x)\n",
        "\n",
        "        # Attention scores\n",
        "        att = (q @ k.transpose(-2, -1)) * (1.0 / (C ** 0.5))\n",
        "        att = t.nn.functional.softmax(att, dim=-1)\n",
        "        att = self.dropout(att)\n",
        "\n",
        "        y = att @ v\n",
        "        return y\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, num_heads, head_size):\n",
        "        super().__init__()\n",
        "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
        "        self.proj = nn.Linear(head_size * num_heads, n_embd)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = t.cat([h(x) for h in self.heads], dim=-1)\n",
        "        out = self.proj(out)\n",
        "        out = self.dropout(out)\n",
        "        return out\n",
        "\n",
        "class FeedForward(nn.Module):\n",
        "    def __init__(self, n_embd):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(n_embd, n_embd * 4),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(n_embd * 4, n_embd),\n",
        "            nn.Dropout(dropout)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "class Block(nn.Module):\n",
        "    def __init__(self, n_embd, n_head):\n",
        "        super().__init__()\n",
        "        head_size = n_embd // n_head\n",
        "        self.sa = MultiHeadAttention(n_head, head_size)\n",
        "        self.ffwd = FeedForward(n_embd)\n",
        "        self.ln1 = nn.LayerNorm(n_embd)\n",
        "        self.ln2 = nn.LayerNorm(n_embd)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.sa(self.ln1(x))\n",
        "        x = x + self.ffwd(self.ln2(x))\n",
        "        return x\n",
        "\n",
        "class NewModel(nn.Module):\n",
        "    def __init__(self, vocab_size, n_embd, n_head, n_layer, block_size):\n",
        "        super().__init__()\n",
        "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
        "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
        "        self.blocks = nn.Sequential(*[Block(n_embd, n_head) for _ in range(n_layer)])\n",
        "        self.ln_f = nn.LayerNorm(n_embd)  # Final layer norm\n",
        "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "        B, T = idx.shape\n",
        "        tok_emb = self.token_embedding_table(idx)  # (B, T, C)\n",
        "        pos_emb = self.position_embedding_table(t.arange(T, device=idx.device))  # (T, C)\n",
        "        x = tok_emb + pos_emb.unsqueeze(0)\n",
        "        x = self.blocks(x)\n",
        "        x = self.ln_f(x)\n",
        "        logits = self.lm_head(x)\n",
        "\n",
        "        loss = None\n",
        "        if targets is not None:\n",
        "            B, T, C = logits.shape\n",
        "            logits = logits.view(B * T, C)\n",
        "            targets = targets.view(B * T)\n",
        "            loss = f.cross_entropy(logits, targets)\n",
        "\n",
        "        return logits, loss\n",
        "\n",
        "    def generate(self, idx, max_new_tokens):\n",
        "        for _ in range(max_new_tokens):\n",
        "            idx_con = idx[:, -block_size:]\n",
        "            logits, _ = self.forward(idx_con)  # Explicitly call forward\n",
        "\n",
        "            logits = logits[:, -1, :]  # Focus only on the last time step\n",
        "            probs = f.softmax(logits, dim=-1)\n",
        "            idx_next = t.multinomial(probs, num_samples=1)\n",
        "\n",
        "            idx = t.cat((idx, idx_next), dim=1)\n",
        "\n",
        "        return idx"
      ],
      "metadata": {
        "id": "YTCGdvTDPcQK"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "kGx8dh4ajKfM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = NewModel(vocab_size=vocab_size,n_embd=n_embd,n_head=n_head,n_layer=n_layer,block_size=block_size)"
      ],
      "metadata": {
        "id": "V-94pN0xLWeq"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Training loop\n",
        "optimizer = t.optim.AdamW(model.parameters(), lr=learning_rate)\n",
        "for iter in range(max_iters):\n",
        "    if iter % eval_interval == 0:\n",
        "        print(f\"Current iteration:{iter}\")\n",
        "    xb, yb = get_batch('train')\n",
        "    # Forward pass\n",
        "    logits, loss = model(xb, yb)\n",
        "    # Backward pass\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()"
      ],
      "metadata": {
        "id": "fzwl4qJmbe4r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(decode(model.generate(idx = t.zeros((1,1),dtype=t.long),max_new_tokens=500)[0].tolist()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lLFyIQjbbzDo",
        "outputId": "5240f43a-5929-4abb-d925-b5473115e245"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "qTNIT\n",
            "-BALIONdbN:\n",
            "I3D:\n",
            "\n",
            "hasth3agethothotonoowoHoooooooooFooooooooooodooooooouooooboo$oooomouoooooooooooofoopooowoooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooomoou wooowozooouoooo?oooooooooo\n",
            "ooooooooooooooooooooo ooooouo oooooomuooowoKososoooosoooooooooooooooooooowoooooonooooooooooooooooooootoooooooooo,oo ooowooooooooooooooooooooooooooooooooooooooooooouooooooooooootoowooooMooooooooooooooooooo;ooooooopooooogoouofooooooooooooooooooooooooooooooooooooooooooogooooooooooocoooooooooo\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "class Head(nn.Module):\n",
        "    def __init__(self, head_size):\n",
        "        super().__init__()\n",
        "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, T, C = x.shape\n",
        "        k = self.key(x)\n",
        "        q = self.query(x)\n",
        "        v = self.value(x)\n",
        "\n",
        "        # Attention scores\n",
        "        att = (q @ k.transpose(-2, -1)) * (1.0 / (C ** 0.5))\n",
        "        att = t.nn.functional.softmax(att, dim=-1)\n",
        "        att = self.dropout(att)\n",
        "\n",
        "        y = att @ v\n",
        "        return y\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, num_heads, head_size):\n",
        "        super().__init__()\n",
        "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
        "        self.proj = nn.Linear(head_size * num_heads, n_embd)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = t.cat([h(x) for h in self.heads], dim=-1)\n",
        "        out = self.proj(out)\n",
        "        out = self.dropout(out)\n",
        "        return out\n",
        "\n",
        "class FeedForward(nn.Module):\n",
        "    def __init__(self, n_embd):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(n_embd, n_embd * 4),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(n_embd * 4, n_embd),\n",
        "            nn.Dropout(dropout)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "class Block(nn.Module):\n",
        "    def __init__(self, n_embd, n_head):\n",
        "        super().__init__()\n",
        "        head_size = n_embd // n_head\n",
        "        self.sa = MultiHeadAttention(n_head, head_size)\n",
        "        self.ffwd = FeedForward(n_embd)\n",
        "        self.ln1 = nn.LayerNorm(n_embd)\n",
        "        self.ln2 = nn.LayerNorm(n_embd)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.sa(self.ln1(x))\n",
        "        x = x + self.ffwd(self.ln2(x))\n",
        "        return x\n",
        "\n",
        "class NewModel(nn.Module):\n",
        "    def __init__(self, vocab_size, n_embd, n_head, n_layer, block_size):\n",
        "        super().__init__()\n",
        "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
        "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
        "        self.blocks = nn.Sequential(*[Block(n_embd, n_head) for _ in range(n_layer)])\n",
        "        self.ln_f = nn.LayerNorm(n_embd)  # Final layer norm\n",
        "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "        B, T = idx.shape\n",
        "        tok_emb = self.token_embedding_table(idx)  # (B, T, C)\n",
        "        pos_emb = self.position_embedding_table(t.arange(T, device=idx.device))  # (T, C)\n",
        "        x = tok_emb + pos_emb.unsqueeze(0)\n",
        "        x = self.blocks(x)\n",
        "        x = self.ln_f(x)\n",
        "        logits = self.lm_head(x)\n",
        "\n",
        "        loss = None\n",
        "        if targets is not None:\n",
        "            B, T, C = logits.shape\n",
        "            logits = logits.view(B * T, C)\n",
        "            targets = targets.view(B * T)\n",
        "            loss = f.cross_entropy(logits, targets)\n",
        "\n",
        "    def generate(self, idx, max_new_tokens):\n",
        "        for _ in range(max_new_tokens):\n",
        "            idx_con = idx[:, -block_size:]\n",
        "            logits, _ = self.forward(idx_con)  # Explicitly call forward\n",
        "\n",
        "            logits = logits[:, -1, :]  # Focus only on the last time step\n",
        "            probs = f.softmax(logits, dim=-1)\n",
        "            idx_next = t.multinomial(probs, num_samples=1)\n",
        "\n",
        "            idx = t.cat((idx, idx_next), dim=1)\n",
        "\n",
        "        return idx\n",
        "\n",
        "# Initialize and test the model\n",
        "chars = sorted(list(set(text)))\n",
        "vocab_size = len(chars)  # Define or load the chars set\n",
        "stoi = {ch: i for i, ch in enumerate(chars)}\n",
        "itos = {i: ch for i, ch in enumerate(chars)}\n",
        "encode = lambda s: [stoi[c] for c in s]\n",
        "decode = lambda l: ''.join([itos[i] for i in l])\n",
        "\n",
        "# Example text for testing\n",
        "\n",
        "\n",
        "data = t.tensor(encode(text), dtype=t.long)\n",
        "\n",
        "# Create training and validation splits\n",
        "split = int(0.9 * len(data))\n",
        "train_data = data[:split]\n",
        "val_data = data[split:]\n",
        "\n",
        "def get_batch(split):\n",
        "    data = train_data if split == 'train' else val_data\n",
        "    ix = t.randint(0, len(data) - block_size, (batch_size,))\n",
        "    x = t.stack([data[i:i+block_size] for i in ix])\n",
        "    y = t.stack([data[i+1:i+block_size+1] for i in ix])\n",
        "    return x, y\n",
        "\n",
        "@t.no_grad()\n",
        "def estimate_loss():\n",
        "    out = {}\n",
        "    model.eval()\n",
        "    for split in ['train', 'val']:\n",
        "        losses = t.zeros(eval_iters)\n",
        "        for k in range(eval_iters):\n",
        "            X, Y = get_batch(split)\n",
        "            logits, loss = model(X, Y)\n",
        "            losses[k] = loss.item()\n",
        "        out[split] = losses.mean()\n",
        "    model.train()\n",
        "    return out\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "oevnF3VenuRS"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = NewModel(vocab_size, n_embd, n_head, n_layer, block_size)\n",
        "optimizer = t.optim.AdamW(model.parameters(), lr=learning_rate)\n",
        "max_iters = 1000\n",
        "from tqdm.auto import tqdm\n",
        "# Training loop\n",
        "for epoch in tqdm(range(max_iters)):\n",
        "    if iter % eval_interval == 0:\n",
        "        print(f\"STEP:{iter}\")\n",
        "\n",
        "    xb, yb = get_batch('train')\n",
        "    xb, yb = xb, yb\n",
        "\n",
        "    # Forward pass\n",
        "    logits, loss = model(xb, yb)\n",
        "    # Backward pass\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "157a7ed218bb4effbe5b8c5595f530b0",
            "7f248edf00354cea8a67ecd74457c812",
            "a557bdef1c054e6e9abc4f84be2ddb6d",
            "09440ab6ee20495bb66d6bf43824229b",
            "7755c3d2929141af987370ee23b94f70",
            "ff3307c0dcf94df7ac0e28efcc9134cd",
            "e88233e03d3849dd9dc6a1e6a1c349a8",
            "dbaf2cfdaad24e409bd0553c7fe91238",
            "9803fb0742544005b460eb02c0a9a0f0",
            "57c5cf46b1fc44f39e641861e70a0198",
            "7ea854867e474a7ba9f73ac59deef013"
          ]
        },
        "id": "iek6FJEVn9OG",
        "outputId": "3a0613fa-53f2-4a8b-cc3a-220d321bdc70"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/1000 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "157a7ed218bb4effbe5b8c5595f530b0"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "context = t.zeros((1, 1), dtype=t.long)\n",
        "print(decode(model.generate(context, max_new_tokens=500)[0].tolist()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GAPVoLjtpCCu",
        "outputId": "398aa5fc-b7e9-4567-f7a4-697bee0b2c07"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "             v      i    i i i ialize vocab_sizeSome example text to initialize vocab_sizeSomeSomextextextomplextext to initialize vocab_sizeSome example text to initialize vocab_sizeSomplexample text to initialize vocab_sizeSome exazexample text to initialize vocab_sizeSomplexample text toxto ininiavocab_sizeSome example text to initialize vocab_sizeSome example text to initialize vocab_sizeSome example text to initialize vocab_sizeSome example text to initialize vocab_sizeSome example text to i\n"
          ]
        }
      ]
    }
  ]
}